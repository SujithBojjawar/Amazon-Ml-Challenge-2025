{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the first pipeline for processing text using GradientBoost\n",
        "* Implemented a baseline text-based pipeline to predict product prices using the training data.\n",
        "* Cleaned product descriptions to remove newlines, extra spaces, and standardize text.\n",
        "* Extracted numeric `Value` and `Unit` from product descriptions to use as features.\n",
        "* Applied one-hot encoding to `Unit` and TF-IDF vectorization to the cleaned text for numerical representation.\n",
        "* Combined numeric, categorical, and text features into a single dataset for modeling.\n",
        "* Trained a **Gradient Boosting Regressor** on this combined dataset.\n",
        "* Predicted prices on a subset of the data and evaluated using **SMAPE**.\n",
        "* Achieved **Subset SMAPE: 36.68%**, serving as a baseline for model performance on the initial pipeline.\n",
        "##Next steps :\n",
        "* We will try XGBoost instead of Gradient Boosting in the next notebook\n",
        "* Reason: XGBoost is often faster and can give better accuracy for tabular data.\n",
        "* We will reuse the same cleaned and processed text + numeric + one-hot features."
      ],
      "metadata": {
        "id": "bs15D-ryOVjg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUztTcslxFLd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\n",
        "    'train.csv',\n",
        "    sep=',',\n",
        "    quotechar='\"',       # handle quoted text\n",
        "    engine='python',     # Python engine handles multiline text\n",
        "    on_bad_lines='skip'  # skip malformed lines\n",
        ")"
      ],
      "metadata": {
        "id": "9LAjT-d93H7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv(\n",
        "    'test.csv',\n",
        "    sep=',',\n",
        "    quotechar='\"',\n",
        "    engine='python',\n",
        "    on_bad_lines='skip'\n",
        ")"
      ],
      "metadata": {
        "id": "urLTa7fr3beS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_small = train.sample(n=1000, random_state=42)  # 1000 rows, change n as needed\n",
        "test_small = test.sample(n=1000, random_state=42)"
      ],
      "metadata": {
        "id": "WDUt3wi93e_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Subset Train shape:\", train_small.shape)\n",
        "print(\"Subset Test shape:\", test_small.shape)\n",
        "print(train_small.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_CKml_3iou",
        "outputId": "fb3d4a8b-632e-456e-bf83-7c16eddb83af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset Train shape: (1000, 4)\n",
            "Subset Test shape: (1000, 3)\n",
            "      sample_id                                    catalog_content  \\\n",
            "282      293950  Item Name: Yakami Orchard Japanese Yuzu Marmal...   \n",
            "2008      80692  Item Name: Sweet Sue Chunk White Chicken in Wa...   \n",
            "1713      65727  Item Name: Octonuts Dry Roasted Maple Almond N...   \n",
            "1666        256  Item Name: POSHI Marinated French Green Bean S...   \n",
            "2066     139960  Item Name: Mars SNICKERS, TWIX, MILKY WAY & 3 ...   \n",
            "\n",
            "                                             image_link  price  \n",
            "282   https://m.media-amazon.com/images/I/41S7nuYFld...  19.95  \n",
            "2008  https://m.media-amazon.com/images/I/31kSwbxAI7...  10.32  \n",
            "1713  https://m.media-amazon.com/images/I/51BD1O+4mR...   9.75  \n",
            "1666  https://m.media-amazon.com/images/I/71h2JMdbvj...   2.15  \n",
            "2066  https://m.media-amazon.com/images/I/61irQQmkZn...  10.00  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Replace newline characters with space\n",
        "    text = text.replace('\\n', ' ')\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "m43Ikl1x4KCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_small['clean_text'] = train_small['catalog_content'].apply(clean_text)\n",
        "test_small['clean_text'] = test_small['catalog_content'].apply(clean_text)"
      ],
      "metadata": {
        "id": "sHV9ZYlR4ZQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_small[['catalog_content', 'clean_text']].head(2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7-covgY4d1G",
        "outputId": "5560227d-4004-4ebd-d314-d10d9d6b5052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        catalog_content  \\\n",
            "282   Item Name: Yakami Orchard Japanese Yuzu Marmal...   \n",
            "2008  Item Name: Sweet Sue Chunk White Chicken in Wa...   \n",
            "\n",
            "                                             clean_text  \n",
            "282   item name: yakami orchard japanese yuzu marmal...  \n",
            "2008  item name: sweet sue chunk white chicken in wa...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_value_unit(text):\n",
        "    \"\"\"\n",
        "    Extract numeric Value and Unit from catalog_content.\n",
        "\n",
        "    Returns:\n",
        "        value (float) or None\n",
        "        unit (str) or None\n",
        "    \"\"\"\n",
        "    value, unit = None, None\n",
        "\n",
        "    # Look for \"Value: <number>\" pattern (allow spaces and decimals)\n",
        "    value_match = re.search(r'Value[: ]+\\s*([\\d\\.]+)', text, re.IGNORECASE)\n",
        "    if value_match:\n",
        "        try:\n",
        "            value = float(value_match.group(1))\n",
        "        except:\n",
        "            value = None\n",
        "\n",
        "    # Look for \"Unit: <word>\" pattern\n",
        "    unit_match = re.search(r'Unit[: ]+\\s*([a-zA-Z]+)', text, re.IGNORECASE)\n",
        "    if unit_match:\n",
        "        unit = unit_match.group(1).lower()\n",
        "\n",
        "    return value, unit\n",
        "\n",
        "# Apply to train and test\n",
        "train_small[['value_num', 'unit']] = train_small['catalog_content'].apply(\n",
        "    lambda x: pd.Series(extract_value_unit(x))\n",
        ")\n",
        "test_small[['value_num', 'unit']] = test_small['catalog_content'].apply(\n",
        "    lambda x: pd.Series(extract_value_unit(x))\n",
        ")\n",
        "\n",
        "# Check results\n",
        "print(train_small[['catalog_content', 'value_num', 'unit']].head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cS4ANFfN4jF9",
        "outputId": "f0fcf225-9f87-4773-d170-e23e403218b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        catalog_content  value_num   unit\n",
            "282   Item Name: Yakami Orchard Japanese Yuzu Marmal...       10.0  ounce\n",
            "2008  Item Name: Sweet Sue Chunk White Chicken in Wa...        6.0  count\n",
            "1713  Item Name: Octonuts Dry Roasted Maple Almond N...       16.0  ounce\n",
            "1666  Item Name: POSHI Marinated French Green Bean S...       17.6  ounce\n",
            "2066  Item Name: Mars SNICKERS, TWIX, MILKY WAY & 3 ...       16.0  ounce\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode categorical 'unit' column\n",
        "train_units = pd.get_dummies(train_small['unit'], prefix='unit', dummy_na=True)\n",
        "test_units = pd.get_dummies(test_small['unit'], prefix='unit', dummy_na=True)\n",
        "\n",
        "# Align test columns to train (some units may not appear in test)\n",
        "test_units = test_units.reindex(columns=train_units.columns, fill_value=0)\n",
        "\n",
        "# Replace unit column with one-hot\n",
        "train_small = pd.concat([train_small, train_units], axis=1)\n",
        "test_small = pd.concat([test_small, test_units], axis=1)"
      ],
      "metadata": {
        "id": "JXIEvhZx5vbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TF-IDF vectorizer, limit to 2000 features for speed\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "\n",
        "# Fit on train, transform both train and test\n",
        "X_train_text = vectorizer.fit_transform(train_small['clean_text'])\n",
        "X_test_text = vectorizer.transform(test_small['clean_text'])"
      ],
      "metadata": {
        "id": "wKfqrJP6_IhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure numeric arrays are float\n",
        "X_train_num = train_small[['value_num']].fillna(0).astype(np.float64).values\n",
        "X_test_num = test_small[['value_num']].fillna(0).astype(np.float64).values\n",
        "\n",
        "# Ensure one-hot categorical arrays are float\n",
        "X_train_cat = train_small[train_units.columns].astype(np.float64).values\n",
        "X_test_cat = test_small[test_units.columns].astype(np.float64).values\n",
        "\n",
        "# Now stack everything\n",
        "X_train = hstack([X_train_text, X_train_num, X_train_cat])\n",
        "X_test = hstack([X_test_text, X_test_num, X_test_cat])"
      ],
      "metadata": {
        "id": "WgXm8rWn_PY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train_small['price'].values"
      ],
      "metadata": {
        "id": "-jIZvRzK_TQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Baseline model\n",
        "model = GradientBoostingRegressor(\n",
        "    n_estimators=200,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train on subset\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test subset\n",
        "y_pred = model.predict(X_test)"
      ],
      "metadata": {
        "id": "ieiMTCF2Cocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Attach predictions to test dataframe\n",
        "output = test_small[['sample_id']].copy()\n",
        "output['price'] = y_pred\n",
        "\n",
        "print(output.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqalTFC8Cr2i",
        "outputId": "d63ecd28-6b18-4c78-f991-553f9ade0b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       sample_id      price\n",
            "10906     259458  14.245655\n",
            "10658      48563  39.111912\n",
            "9500      148194  14.404646\n",
            "16799     185297   9.869420\n",
            "10324     232138  19.148389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only for subset with actual price (train_small for testing)\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "def smape(y_true, y_pred):\n",
        "    return 100 * np.mean(np.abs(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2))\n",
        "\n",
        "# Evaluate on subset (train_small itself or holdout split)\n",
        "smape_score = smape(train_small['price'], model.predict(X_train))\n",
        "print(f\"Subset SMAPE: {smape_score:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3mIObPjCyju",
        "outputId": "373e424d-0a1e-412e-9801-cd9c9035e836"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset SMAPE: 36.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQZCvVN9C2_y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}